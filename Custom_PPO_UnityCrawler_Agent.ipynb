{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abce5950",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8278d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union, Any\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import VecTransposeImage\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3b75f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd github & git clone https://github.com/mwydmuch/ViZDoom.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75813f9",
   "metadata": {},
   "source": [
    "For this project we will be making use of openai gym to train our agents in a Unity game environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf16cd3",
   "metadata": {},
   "source": [
    "### Import Unity gym dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd58c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_unity.envs import UnityToGymWrapper\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.registry import default_registry\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c931926",
   "metadata": {},
   "source": [
    "### Game Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f9a7d6",
   "metadata": {},
   "source": [
    "We pass in the file path that contains the game executable file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "144518c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_path = \"C:/Users/Jolomi/Downloads/CCT 4th Year/Capstone/Unity_ML/UL_ML/TestBuild_ML2/UL_ML.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8470a8",
   "metadata": {},
   "source": [
    "We will be training our agent in a Unity engine environment using customized algorithms, to do this we need to \"wrap\" the game with openai gym wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7f7ee",
   "metadata": {},
   "source": [
    "This enables us to collect observations from game along with rewards and pass it to our agent. This in turn enables our agent to perform actions in the game world which will be how our agent learns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed739e",
   "metadata": {},
   "source": [
    "### Wrapping and defining our environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc9c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityGymCrawler(Env):\n",
    "    def __init__(self, render=False):\n",
    "        super(UnityGymCrawler, self).__init__()\n",
    "        \n",
    "        #Used for modifying the unity environment\n",
    "        channel = EngineConfigurationChannel()\n",
    "        \n",
    "        #Our Unity script is written in C# while most RL algorithms are in python \n",
    "        #We will use the UnityEnvironment wrapper to be able to communicate with the\n",
    "        #unity environment using python.       \n",
    "        #We will also decide if we want to render game environment while training. ,side_channels=[channel]\n",
    "        if render == False:\n",
    "            env = UnityEnvironment(env_path, worker_id = 5, no_graphics=True)\n",
    "        else:\n",
    "            env = UnityEnvironment(env_path, worker_id = 0,side_channels=[channel])       \n",
    "        \n",
    "        \n",
    "        #We change the time scale of the game using a unity environment side channel\n",
    "        #This enables us to speed up the learning process but the physics in the game may perform unpredictably\n",
    "        channel.set_configuration_parameters(time_scale = 2.0)\n",
    "        #Wrapping the python unity environment so that we can use it in openai gym\n",
    "        env = UnityToGymWrapper(env, allow_multiple_obs=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #We define the action space and size as well as the observation space\n",
    "        #this allows our RL algorithms to effectivly communicate with the environment\n",
    "        self.env = env\n",
    "        self.action_space = self.env.action_space\n",
    "        self.action_size = self.env.action_size\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(1,32), dtype=np.uint8)       \n",
    "\n",
    "    #reset the environment \n",
    "    def reset(self):       \n",
    "        return self.env.reset()\n",
    "    #This moves the game foward and returns the game state s, reward r\n",
    "    #done d(to indicate if the game is done) and information info from the game such as player lives\n",
    "    def step(self, action):\n",
    "        s, r, d, info = self.env.step(action)\n",
    "        return s, float(r), d, info\n",
    "    #Closes the environment\n",
    "    def close(self):\n",
    "        #self.env.close()\n",
    "        pass\n",
    "    #render the environment\n",
    "    def render(self, mode=\"human\"):\n",
    "        #self.env.render()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "574ad4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityGymWorm(Env):\n",
    "    def __init__(self, render=False):\n",
    "        super(UnityGymWorm, self).__init__()\n",
    "        \n",
    "        #Used for modifying the unity environment\n",
    "        channel = EngineConfigurationChannel()\n",
    "        \n",
    "        #Our Unity script is written in C# while most RL algorithms are in python \n",
    "        #We will use the UnityEnvironment wrapper to be able to communicate with the\n",
    "        #unity environment using python.       \n",
    "        #We will also decide if we want to render game environment while training. ,side_channels=[channel]\n",
    "        if render == False:\n",
    "            env = UnityEnvironment(env_path, worker_id = 0,side_channels=[channel], no_graphics=True)\n",
    "        else:\n",
    "            env = UnityEnvironment(env_path, worker_id = 0,side_channels=[channel])       \n",
    "        \n",
    "        \n",
    "        #We change the time scale of the game using a unity environment side channel\n",
    "        #This enables us to speed up the learning process but the physics in the game may perform unpredictably\n",
    "        channel.set_configuration_parameters(time_scale = 2.0)\n",
    "        #Wrapping the python unity environment so that we can use it in openai gym\n",
    "        env = UnityToGymWrapper(env, allow_multiple_obs=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #We define the action space and size as well as the observation space\n",
    "        #this allows our RL algorithms to effectivly communicate with the environment\n",
    "        self.env = env\n",
    "        self.action_space = self.env.action_space\n",
    "        self.action_size = self.env.action_size\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(1,64), dtype=np.uint8)       \n",
    "\n",
    "    #reset the environment \n",
    "    def reset(self):       \n",
    "        return self.env.reset()\n",
    "    #This moves the game foward and returns the game state s, reward r\n",
    "    #done d(to indicate if the game is done) and information info from the game such as player lives\n",
    "    def step(self, action):\n",
    "        s, r, d, info = self.env.step(action)\n",
    "        return s, float(r), d, info\n",
    "    #Closes the environment\n",
    "    def close(self):\n",
    "        #self.env.close()\n",
    "        pass\n",
    "    #render the environment\n",
    "    def render(self, mode=\"human\"):\n",
    "        #self.env.render()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "46e0baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f787ff",
   "metadata": {},
   "source": [
    "### Defining our Custom Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0983865",
   "metadata": {},
   "source": [
    "Creating a neural network for our Actor and Critic network, this will be the brains of our agent enabling it to learn and take action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "886d7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork(nn.Module):\n",
    "  \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 32,\n",
    "        last_layer_dim_vf: int = 32,        \n",
    "    ):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi        \n",
    "        self.latent_dim_vf = last_layer_dim_vf      \n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()            \n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()            \n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \n",
    "        return self.policy_net(features), self.value_net(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37236afc",
   "metadata": {},
   "source": [
    "### Defining our Custom Actor-Critic Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0941e",
   "metadata": {},
   "source": [
    "We create policies for the actor-critic network, here we can specify the number of neurons and size of our the hidden layers\n",
    "in our networks using net_arch and how many are shared amoung the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6765f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: gym.spaces.Space,\n",
    "        action_space: gym.spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n",
    "        #net_arch = [dict(pi=[32, 32, 32], vf=[32, 32, 32])],\n",
    "        activation_fn: Type[nn.Module] = th.nn.ReLU,\n",
    "        #activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super(CustomActorCriticPolicy, self).__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Disable orthogonal initialization\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60300aaa",
   "metadata": {},
   "source": [
    "### Setting up Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f9a85",
   "metadata": {},
   "source": [
    " We pass in how frequently we want to save our model, were we are going to be saving it and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3d6d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAndLoggingCallback(BaseCallback):\n",
    "    \n",
    "   \n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainingAndLoggingCallback, self). __init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54128bb7",
   "metadata": {},
   "source": [
    "Create folders for saving our models and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6979927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_basic6'\n",
    "LOG_DIR = './train/log_basic6'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c461460",
   "metadata": {},
   "source": [
    "Create an instance of our train and logging callbacks. After the specified training steps, we will save the best version of our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32267deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainingAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e47314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed002f8",
   "metadata": {},
   "source": [
    "### Train our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7fde2b",
   "metadata": {},
   "source": [
    "Create the video game environment without rendering it to save compute power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55fb55cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 1.5.0-preview and communication version 1.2.0\n",
      "[INFO] Connected new brain: WormStatic?team=0\n"
     ]
    }
   ],
   "source": [
    "env = UnityGymWorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "84bcd216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 1.5.0-preview and communication version 1.2.0\n",
      "[INFO] Connected new brain: WormStatic?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jolomi\\anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "#env = UnityGymCrawler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089cba9",
   "metadata": {},
   "source": [
    "We now create our model using our custom neural network and policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a1ea4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(CustomActorCriticPolicy, env, tensorboard_log=LOG_DIR, verbose=1) , policy_kwargs=policy_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "41dde5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "#model = PPO(\"MlpPolicy\", env, tensorboard_log=LOG_DIR, verbose=1, learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bde2ab",
   "metadata": {},
   "source": [
    "Train our model using a specified number of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7d506323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./train/log_basic6\\PPO_7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-04d2ba210c2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    297\u001b[0m     ) -> \"PPO\":\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         return super(PPO, self).learn(\n\u001b[0m\u001b[0;32m    300\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[0mnew_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \"\"\"\n\u001b[0;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0m\u001b[0;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\monitor.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-147-ec7bb25dd4f4>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m#done d(to indicate if the game is done) and information info from the game such as player lives\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m#Closes the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym_unity\\envs\\__init__.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         \u001b[0mdecision_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_agents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecision_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterminal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mlagents_envs\\timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mlagents_envs\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[0mstep_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"communicator.exchange\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll_process\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityCommunicatorStoppedException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Communicator has exited.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[1;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll_for_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoll_callback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mpoll_for_timeout\u001b[1;34m(self, poll_callback)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mcallback_timeout_wait\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout_wait\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback_timeout_wait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m                 \u001b[1;31m# Got an acknowledgment from the connection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    877\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[0mready_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_exhaustive_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaithandle_to_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForMultipleObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=60000,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f037358",
   "metadata": {},
   "source": [
    "The model appears to be very unstable and unable to learn. We can imporve our model by altering between various hyperparameters like the learning rate, gamma and n_steps, we can also manually alter the number of neurons and hidden layers in our networks to find the best combination. \n",
    "\n",
    "However, doing all of this manually is going to be very time consuming and tedious. So, we will automate our hyperparameter search using Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece442b1",
   "metadata": {},
   "source": [
    "## Optuna for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241b37b",
   "metadata": {},
   "source": [
    "### Dependencies for optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ed3de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd2e6c",
   "metadata": {},
   "source": [
    "### Optuna variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190fff5",
   "metadata": {},
   "source": [
    "We specify variables used by optuna, including the number of trails to be performed to find the ideal hyperparameters, policy etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4df22450",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 100\n",
    "N_STARTUP_TRIALS = 10\n",
    "N_EVALUATIONS = 2\n",
    "N_TIMESTEPS = int(2e4)\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_EPISODES = 3\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": env,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ff87b",
   "metadata": {},
   "source": [
    "### Optuna Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af6726c",
   "metadata": {},
   "source": [
    "In our sampler we specify the range of values for the various hyperparameters that Optuna is going to search in order to find an ideal combination that yields a stable agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ee1cf",
   "metadata": {},
   "source": [
    "We also specify various configurations of neural networks for Optuna to search, this includes different number of neurons and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1127a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_PPO_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "    gae_lambda = 1.0 - trial.suggest_float(\"gae_lambda\", 0.001, 0.2, log=True)\n",
    "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n",
    "    ortho_init = trial.suggest_categorical(\"ortho_init\", [False, True])\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"gae_lambda_\", gae_lambda)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    net_arch = [\n",
    "        {\"pi\": [32], \"vf\": [32]} if net_arch == \"tiny\"\n",
    "        else {\"pi\": [32, 32], \"vf\": [32, 32]}   \n",
    "    ]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "            \"ortho_init\": ortho_init,\n",
    "        },\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc937f6",
   "metadata": {},
   "source": [
    "### Callback for evaluating trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeca2ad",
   "metadata": {},
   "source": [
    "Callback to be used in evaluating and testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5a20b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialEvalCallback(EvalCallback):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833b48b",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956becdc",
   "metadata": {},
   "source": [
    "Using how specified hyperparameter and neural network architecture ranges, Optuna will not run a number of trials to find the best combination that produces the best possible agent.\n",
    "\n",
    "After a stabel agent is found and Optuna determines that a better model is no longer possible, the trail is stopped and the  hyperparameters and network architecture values of the best model is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "554e7400",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-21 22:37:44,008]\u001b[0m A new study created in memory with name: no-name-eb22b4b9-1357-461e-879d-a915358c053d\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 22:52:38,616]\u001b[0m Trial 0 finished with value: 7.666666666666667e-06 and parameters: {'gamma': 0.0001536317694767627, 'max_grad_norm': 0.5775432638324483, 'gae_lambda': 0.023255323482526007, 'exponent_n_steps': 9, 'lr': 0.012086094807391527, 'ent_coef': 0.0010235593565421405, 'ortho_init': True, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 0 with value: 7.666666666666667e-06.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  1\n",
      "Best trial:\n",
      "  Value:  7.666666666666667e-06\n",
      "  Params: \n",
      "    gamma: 0.0001536317694767627\n",
      "    max_grad_norm: 0.5775432638324483\n",
      "    gae_lambda: 0.023255323482526007\n",
      "    exponent_n_steps: 9\n",
      "    lr: 0.012086094807391527\n",
      "    ent_coef: 0.0010235593565421405\n",
      "    ortho_init: True\n",
      "    net_arch: tiny\n",
      "    activation_fn: relu\n",
      "  User attrs:\n",
      "    gamma_: 0.9998463682305232\n",
      "    gae_lambda_: 0.976744676517474\n",
      "    n_steps: 512\n"
     ]
    }
   ],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    # Sample hyperparameters\n",
    "    kwargs.update(sample_PPO_params(trial))\n",
    "    # Create our model\n",
    "    model = PPO(**kwargs)\n",
    "    # Wrapping env used for evaluation\n",
    "    eval_env = Monitor(env)\n",
    "    eval_env = DummyVecEnv([lambda: eval_env])\n",
    "    #eval_env = VecTransposeImage(eval_env)    \n",
    "    # Create the callback that will periodically evaluate\n",
    "    # and report the performance\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set pytorch num threads to 1 for faster training\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "    # Do not prune before 1/3 of the max budget is used\n",
    "    pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3)\n",
    "\n",
    "    study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=N_TRIALS, timeout=600)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    print(\"  User attrs:\")\n",
    "    for key, value in trial.user_attrs.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed1d36c",
   "metadata": {},
   "source": [
    "A stable model was found after only two trails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46ab0e",
   "metadata": {},
   "source": [
    "### Create and train our tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225db16b",
   "metadata": {},
   "source": [
    "Having completed our hyperparameter search, we plug in the hyperparameters generated from the search into our model along with the network archtecture values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807588e",
   "metadata": {},
   "source": [
    "We supply the searched network architecture, orth_init and activation function using policy kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7248eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs2 = dict(\n",
    "    ortho_init = True,\n",
    "    activation_fn = th.nn.ReLU,\n",
    "    net_arch = [dict(pi=[32, 32], vf=[32, 32])],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c971b06",
   "metadata": {},
   "source": [
    "Create our model using searched hyperparameters and policy kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e0e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"CnnPolicy\", env, tensorboard_log=LOG_DIR, verbose=1,\n",
    "           gamma = 0.9171858127315068, max_grad_norm = 1.5061594927147473,\n",
    "           gae_lambda = 0.9973635408305496, learning_rate = 0.000459048881080786,\n",
    "           ent_coef = 2.4386279564585447e-08, policy_kwargs=policy_kwargs2, n_steps = 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7151eb",
   "metadata": {},
   "source": [
    "Train our agent using a specified number of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39cd6fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./train/log_basic\\PPO_40\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.9     |\n",
      "|    ep_rew_mean     | 38.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 98       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.93       |\n",
      "|    ep_rew_mean          | 73         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 79         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 2048       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07594001 |\n",
      "|    clip_fraction        | 0.382      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.662     |\n",
      "|    explained_variance   | 0.584      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 90.4       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | 0.00254    |\n",
      "|    value_loss           | 388        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.52       |\n",
      "|    ep_rew_mean          | 80.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 66         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 46         |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04965913 |\n",
      "|    clip_fraction        | 0.435      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.616     |\n",
      "|    explained_variance   | 0.721      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 185        |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.0292    |\n",
      "|    value_loss           | 590        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.53       |\n",
      "|    ep_rew_mean          | 84.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 66         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07275669 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.425     |\n",
      "|    explained_variance   | 0.468      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 61.2       |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    value_loss           | 261        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.69       |\n",
      "|    ep_rew_mean          | 83.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 62         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 82         |\n",
      "|    total_timesteps      | 5120       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09613909 |\n",
      "|    clip_fraction        | 0.22       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.292     |\n",
      "|    explained_variance   | 0.703      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 36.5       |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.00806   |\n",
      "|    value_loss           | 98.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.39        |\n",
      "|    ep_rew_mean          | 89.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026881834 |\n",
      "|    clip_fraction        | 0.0926      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.624       |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | 48.6        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 180         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.86        |\n",
      "|    ep_rew_mean          | 87.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052020796 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.102      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | 18.5        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.0166      |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.72        |\n",
      "|    ep_rew_mean          | 84          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093100175 |\n",
      "|    clip_fraction        | 0.0724      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0798     |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | 4.09        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00399    |\n",
      "|    value_loss           | 13.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.83        |\n",
      "|    ep_rew_mean          | 83.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061666433 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | 0.000291    |\n",
      "|    value_loss           | 80          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.82       |\n",
      "|    ep_rew_mean          | 87.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 166        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16479266 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0601    |\n",
      "|    explained_variance   | 0.901      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 12.9       |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    value_loss           | 32.8       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.01       |\n",
      "|    ep_rew_mean          | 86.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 183        |\n",
      "|    total_timesteps      | 11264      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09675221 |\n",
      "|    clip_fraction        | 0.0638     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.046     |\n",
      "|    explained_variance   | 0.788      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 32.9       |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | 0.0228     |\n",
      "|    value_loss           | 74.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.36       |\n",
      "|    ep_rew_mean          | 89.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 198        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16132867 |\n",
      "|    clip_fraction        | 0.064      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0372    |\n",
      "|    explained_variance   | 0.874      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 8.82       |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0033    |\n",
      "|    value_loss           | 31.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.26       |\n",
      "|    ep_rew_mean          | 85.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 62         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 214        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03704025 |\n",
      "|    clip_fraction        | 0.0444     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0405    |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 1.45       |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | 0.0156     |\n",
      "|    value_loss           | 14.7       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.77       |\n",
      "|    ep_rew_mean          | 87.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 62         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 230        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06985583 |\n",
      "|    clip_fraction        | 0.0715     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0509    |\n",
      "|    explained_variance   | 0.769      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 13.6       |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.00553   |\n",
      "|    value_loss           | 79.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.88       |\n",
      "|    ep_rew_mean          | 87.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 62         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 246        |\n",
      "|    total_timesteps      | 15360      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03757565 |\n",
      "|    clip_fraction        | 0.0332     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.034     |\n",
      "|    explained_variance   | 0.876      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 20.7       |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | 0.00463    |\n",
      "|    value_loss           | 36.2       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.36       |\n",
      "|    ep_rew_mean          | 85.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 62         |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 263        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07537817 |\n",
      "|    clip_fraction        | 0.0485     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0276    |\n",
      "|    explained_variance   | 0.774      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 75.6       |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    value_loss           | 88.7       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.87       |\n",
      "|    ep_rew_mean          | 87.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 281        |\n",
      "|    total_timesteps      | 17408      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06296997 |\n",
      "|    clip_fraction        | 0.0538     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0518    |\n",
      "|    explained_variance   | 0.885      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 4.82       |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | 0.00315    |\n",
      "|    value_loss           | 25.4       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.1        |\n",
      "|    ep_rew_mean          | 86.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 60         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 302        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12879433 |\n",
      "|    clip_fraction        | 0.0709     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0473    |\n",
      "|    explained_variance   | 0.945      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 3.26       |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | 0.00321    |\n",
      "|    value_loss           | 16.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.22        |\n",
      "|    ep_rew_mean          | 85.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040342927 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0423     |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | 24.5        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | 0.00734     |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.16       |\n",
      "|    ep_rew_mean          | 86.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 58         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 351        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15453027 |\n",
      "|    clip_fraction        | 0.056      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0325    |\n",
      "|    explained_variance   | 0.803      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 11.5       |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.00124   |\n",
      "|    value_loss           | 45.3       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2ca14cfcf40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac061ff",
   "metadata": {},
   "source": [
    "The training is now stable and our agent has learned to maximize its reward as can be seen from the high ep_rew_mean which is the average reward the agent got per episode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2a5b0",
   "metadata": {},
   "source": [
    "### Test and evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a06bd1",
   "metadata": {},
   "source": [
    "We will evaluate our model in the video game environment to determine its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a32698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import evaluate policy to test our agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf11f0d",
   "metadata": {},
   "source": [
    "We load our best trained model to use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "157c5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./train/train_basic2/best_model_80000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91bd03",
   "metadata": {},
   "source": [
    "Create and render our environment to see the agents performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4d9a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ff17a36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 71.0 is 0\n",
      "Total Reward for episode 95.0 is 1\n",
      "Total Reward for episode 95.0 is 2\n",
      "Total Reward for episode 95.0 is 3\n",
      "Total Reward for episode 95.0 is 4\n",
      "Total Reward for episode 95.0 is 5\n",
      "Total Reward for episode 95.0 is 6\n",
      "Total Reward for episode 83.0 is 7\n",
      "Total Reward for episode 95.0 is 8\n",
      "Total Reward for episode 79.0 is 9\n"
     ]
    }
   ],
   "source": [
    "#loop through each game\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.20)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(total_reward, episode))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e043b0",
   "metadata": {},
   "source": [
    "The agent is fully trained and performs very well as seen from the high rewards above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac711d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f32513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
