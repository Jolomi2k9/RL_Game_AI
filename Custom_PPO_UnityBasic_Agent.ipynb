{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abce5950",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8278d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union, Any\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import VecTransposeImage\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b75f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd github & git clone https://github.com/mwydmuch/ViZDoom.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75813f9",
   "metadata": {},
   "source": [
    "For this project we will be making use of openai gym to train our agents in a Unity game environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf16cd3",
   "metadata": {},
   "source": [
    "### Import Unity gym dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd58c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_unity.envs import UnityToGymWrapper\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.registry import default_registry\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c931926",
   "metadata": {},
   "source": [
    "### Game Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f9a7d6",
   "metadata": {},
   "source": [
    "We pass in the file path that contains the game executable file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "144518c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_path = \"C:/Users/Jolomi/Downloads/CCT 4th Year/Capstone/Unity_ML/UL_ML/TestBuild_Basic/UL_ML.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8470a8",
   "metadata": {},
   "source": [
    "We will be training our agent in a Unity engine environment using customized algorithms, to do this we need to \"wrap\" the game with openai gym wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7f7ee",
   "metadata": {},
   "source": [
    "This enables us to collect observations from game along with rewards and pass it to our agent. This in turn enables our agent to perform actions in the game world which will be how our agent learns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed739e",
   "metadata": {},
   "source": [
    "### Wrapping and defining our environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17c4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityGymBasic(Env):\n",
    "    def __init__(self, render=False):\n",
    "        super(UnityGymBasic, self).__init__()\n",
    "        \n",
    "        #Used for modifying the unity environment\n",
    "        channel = EngineConfigurationChannel()\n",
    "        \n",
    "        #Our Unity script is written in C# while most RL algorithms are in python \n",
    "        #We will use the UnityEnvironment wrapper to be able to communicate with the\n",
    "        #unity environment using python.       \n",
    "        #We will also decide if we want to render game environment while training. ,side_channels=[channel]\n",
    "        if render == False:\n",
    "            env = UnityEnvironment(env_path, worker_id = 0,side_channels=[channel], no_graphics=True)\n",
    "        else:\n",
    "            env = UnityEnvironment(env_path, worker_id = 0,side_channels=[channel])       \n",
    "        \n",
    "        \n",
    "        #We change the time scale of the game using a unity environment side channel\n",
    "        #This enables us to speed up the learning process but the physics in the game may perform unpredictably\n",
    "        channel.set_configuration_parameters(time_scale = 2.0)\n",
    "        #Wrapping the python unity environment so that we can use it in openai gym\n",
    "        env = UnityToGymWrapper(env, allow_multiple_obs=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #We define the action space and size as well as the observation space\n",
    "        #this allows our RL algorithms to effectivly communicate with the environment\n",
    "        self.env = env\n",
    "        self.action_space = self.env.action_space\n",
    "        self.action_size = self.env.action_size\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(1,20), dtype=np.uint8)       \n",
    "\n",
    "    #reset the environment \n",
    "    def reset(self):       \n",
    "        return self.env.reset()\n",
    "    #This moves the game foward and returns the game state s, reward r\n",
    "    #done d(to indicate if the game is done) and information info from the game such as player lives\n",
    "    def step(self, action):\n",
    "        s, r, d, info = self.env.step(action)\n",
    "        return s, float(r), d, info\n",
    "    #Closes the environment\n",
    "    def close(self):\n",
    "        #self.env.close()\n",
    "        pass\n",
    "    #render the environment\n",
    "    def render(self, mode=\"human\"):\n",
    "        #self.env.render()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e0baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f787ff",
   "metadata": {},
   "source": [
    "### Defining our Custom Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0983865",
   "metadata": {},
   "source": [
    "Creating a neural network for our Actor and Critic network, this will be the brains of our agent enabling it to learn and take action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "886d7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork(nn.Module):\n",
    "  \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 32,\n",
    "        last_layer_dim_vf: int = 32,        \n",
    "    ):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi        \n",
    "        self.latent_dim_vf = last_layer_dim_vf      \n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()            \n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()            \n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \n",
    "        return self.policy_net(features), self.value_net(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37236afc",
   "metadata": {},
   "source": [
    "### Defining our Custom Actor-Critic Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0941e",
   "metadata": {},
   "source": [
    "We create policies for the actor-critic network, here we can specify the number of neurons and size of our the hidden layers\n",
    "in our networks using net_arch and how many are shared amoung the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6765f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: gym.spaces.Space,\n",
    "        action_space: gym.spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n",
    "        #net_arch = [dict(pi=[32, 32, 32], vf=[32, 32, 32])],\n",
    "        activation_fn: Type[nn.Module] = th.nn.ReLU,\n",
    "        #activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super(CustomActorCriticPolicy, self).__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Disable orthogonal initialization\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60300aaa",
   "metadata": {},
   "source": [
    "### Setting up Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f9a85",
   "metadata": {},
   "source": [
    " We pass in how frequently we want to save our model, were we are going to be saving it and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d6d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAndLoggingCallback(BaseCallback):\n",
    "    \n",
    "   \n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainingAndLoggingCallback, self). __init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54128bb7",
   "metadata": {},
   "source": [
    "Create folders for saving our models and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6979927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_Ubasic3'\n",
    "LOG_DIR = './train/log_Ubasic3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c461460",
   "metadata": {},
   "source": [
    "Create an instance of our train and logging callbacks. After the specified training steps, we will save the best version of our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32267deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainingAndLoggingCallback(check_freq=5000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e47314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed002f8",
   "metadata": {},
   "source": [
    "### Train our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7fde2b",
   "metadata": {},
   "source": [
    "Create the video game environment without rendering it to save compute power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55fb55cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 1.5.0-preview and communication version 1.2.0\n",
      "[INFO] Connected new brain: Basic?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jolomi\\anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "env = UnityGymBasic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089cba9",
   "metadata": {},
   "source": [
    "We now create our model using our custom neural network and policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a1ea4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(CustomActorCriticPolicy, env, tensorboard_log=LOG_DIR, verbose=1) , policy_kwargs=policy_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bde2ab",
   "metadata": {},
   "source": [
    "Train our model using a specified number of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d506323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.learn(total_timesteps=60000,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f037358",
   "metadata": {},
   "source": [
    "The model appears to be very unstable and unable to learn. We can imporve our model by altering between various hyperparameters like the learning rate, gamma and n_steps, we can also manually alter the number of neurons and hidden layers in our networks to find the best combination. \n",
    "\n",
    "However, doing all of this manually is going to be very time consuming and tedious. So, we will automate our hyperparameter search using Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece442b1",
   "metadata": {},
   "source": [
    "## Optuna for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241b37b",
   "metadata": {},
   "source": [
    "### Dependencies for optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ed3de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd2e6c",
   "metadata": {},
   "source": [
    "### Optuna variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190fff5",
   "metadata": {},
   "source": [
    "We specify variables used by optuna, including the number of trails to be performed to find the ideal hyperparameters, policy etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df22450",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 100\n",
    "N_STARTUP_TRIALS = 10\n",
    "N_EVALUATIONS = 2\n",
    "N_TIMESTEPS = int(2e4)\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_EPISODES = 3\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": env,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ff87b",
   "metadata": {},
   "source": [
    "### Optuna Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af6726c",
   "metadata": {},
   "source": [
    "In our sampler we specify the range of values for the various hyperparameters that Optuna is going to search in order to find an ideal combination that yields a stable agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ee1cf",
   "metadata": {},
   "source": [
    "We also specify various configurations of neural networks for Optuna to search, this includes different number of neurons and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1127a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_PPO_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "    gae_lambda = 1.0 - trial.suggest_float(\"gae_lambda\", 0.001, 0.2, log=True)\n",
    "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n",
    "    ortho_init = trial.suggest_categorical(\"ortho_init\", [False, True])\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"gae_lambda_\", gae_lambda)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    net_arch = [\n",
    "        {\"pi\": [16,32], \"vf\": [16,32]} if net_arch == \"tiny\"\n",
    "        else {\"pi\": [32, 64], \"vf\": [32, 64]}   \n",
    "    ]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "            \"ortho_init\": ortho_init,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcb2fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_PPO_params2(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \n",
    "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)    \n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "        \n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,        \n",
    "        \"learning_rate\": learning_rate,        \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc937f6",
   "metadata": {},
   "source": [
    "### Callback for evaluating trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeca2ad",
   "metadata": {},
   "source": [
    "Callback to be used in evaluating and testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5a20b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialEvalCallback(EvalCallback):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833b48b",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956becdc",
   "metadata": {},
   "source": [
    "Using how specified hyperparameter and neural network architecture ranges, Optuna will not run a number of trials to find the best combination that produces the best possible agent.\n",
    "\n",
    "After a stabel agent is found and Optuna determines that a better model is no longer possible, the trail is stopped and the  hyperparameters and network architecture values of the best model is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "554e7400",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-26 03:04:38,233]\u001b[0m A new study created in memory with name: no-name-f8dcd85a-c444-4ecf-ac19-018a995f5fb3\u001b[0m\n",
      "C:\\Users\\Jolomi\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:137: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 16`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 16\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=16 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-04-26 03:08:03,285]\u001b[0m Trial 0 finished with value: 0.93 and parameters: {'gamma': 0.005245929174048228, 'max_grad_norm': 1.0468939329112599, 'gae_lambda': 0.005958657081960792, 'exponent_n_steps': 4, 'lr': 7.036650733476216e-05, 'ent_coef': 0.05914596552250845, 'ortho_init': False, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 0 with value: 0.93.\u001b[0m\n",
      "\u001b[32m[I 2022-04-26 03:11:19,948]\u001b[0m Trial 1 finished with value: 0.9433333333333334 and parameters: {'gamma': 0.038685080655939875, 'max_grad_norm': 0.3872470382434406, 'gae_lambda': 0.043615980722675196, 'exponent_n_steps': 8, 'lr': 0.019090836076901457, 'ent_coef': 0.00013753975042183809, 'ortho_init': False, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 1 with value: 0.9433333333333334.\u001b[0m\n",
      "\u001b[32m[I 2022-04-26 03:14:44,956]\u001b[0m Trial 2 finished with value: 0.9466666666666668 and parameters: {'gamma': 0.00302296183757531, 'max_grad_norm': 4.912725806362835, 'gae_lambda': 0.005802807891653718, 'exponent_n_steps': 10, 'lr': 0.0014105987308725308, 'ent_coef': 8.706395692974635e-06, 'ortho_init': True, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 2 with value: 0.9466666666666668.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  3\n",
      "Best trial:\n",
      "  Value:  0.9466666666666668\n",
      "  Params: \n",
      "    gamma: 0.00302296183757531\n",
      "    max_grad_norm: 4.912725806362835\n",
      "    gae_lambda: 0.005802807891653718\n",
      "    exponent_n_steps: 10\n",
      "    lr: 0.0014105987308725308\n",
      "    ent_coef: 8.706395692974635e-06\n",
      "    ortho_init: True\n",
      "    net_arch: small\n",
      "    activation_fn: relu\n",
      "  User attrs:\n",
      "    gamma_: 0.9969770381624247\n",
      "    gae_lambda_: 0.9941971921083462\n",
      "    n_steps: 1024\n"
     ]
    }
   ],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    # Sample hyperparameters\n",
    "    kwargs.update(sample_PPO_params(trial))\n",
    "    # Create our model\n",
    "    model = PPO(**kwargs)\n",
    "    # Wrapping env used for evaluation\n",
    "    eval_env = Monitor(env)\n",
    "    eval_env = DummyVecEnv([lambda: eval_env])\n",
    "    #eval_env = VecTransposeImage(eval_env)    \n",
    "    # Create the callback that will periodically evaluate\n",
    "    # and report the performance\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set pytorch num threads to 1 for faster training\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "    # Do not prune before 1/3 of the max budget is used\n",
    "    pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3)\n",
    "\n",
    "    study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=N_TRIALS, timeout=600)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    print(\"  User attrs:\")\n",
    "    for key, value in trial.user_attrs.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed1d36c",
   "metadata": {},
   "source": [
    "A stable model was found after only two trails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46ab0e",
   "metadata": {},
   "source": [
    "### Create and train our tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225db16b",
   "metadata": {},
   "source": [
    "Having completed our hyperparameter search, we plug in the hyperparameters generated from the search into our model along with the network archtecture values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807588e",
   "metadata": {},
   "source": [
    "We supply the searched network architecture, orth_init and activation function using policy kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7248eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs2 = dict(\n",
    "    ortho_init = True,\n",
    "    activation_fn = th.nn.ReLU,\n",
    "    net_arch = [dict(pi=[32, 64], vf=[32, 64])],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c971b06",
   "metadata": {},
   "source": [
    "Create our model using searched hyperparameters and policy kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e0e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, tensorboard_log=LOG_DIR, verbose=1,\n",
    "           gamma = 0.9969770381624247, max_grad_norm = 4.912725806362835,\n",
    "           gae_lambda = 0.9941971921083462, learning_rate = 0.0014105987308725308,\n",
    "           ent_coef = 8.706395692974635e-06, policy_kwargs=policy_kwargs2, n_steps = 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7151eb",
   "metadata": {},
   "source": [
    "Train our agent using a specified number of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39cd6fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./train/log_Ubasic3\\PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 43.8     |\n",
      "|    ep_rew_mean     | -0.0252  |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.7        |\n",
      "|    ep_rew_mean          | 0.253       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014134648 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.0305     |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | 0.0566      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 20.6      |\n",
      "|    ep_rew_mean          | 0.524     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 91        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 33        |\n",
      "|    total_timesteps      | 3072      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0317338 |\n",
      "|    clip_fraction        | 0.37      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.03     |\n",
      "|    explained_variance   | 0.263     |\n",
      "|    learning_rate        | 0.00141   |\n",
      "|    loss                 | 0.0703    |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.0388   |\n",
      "|    value_loss           | 0.0892    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.3        |\n",
      "|    ep_rew_mean          | 0.748       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029059695 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.0672     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 0.0448      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12          |\n",
      "|    ep_rew_mean          | 0.871       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033116736 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.813      |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.0312     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0394     |\n",
      "|    value_loss           | 0.0232      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.98        |\n",
      "|    ep_rew_mean          | 0.9         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027783277 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.673      |\n",
      "|    explained_variance   | -0.406      |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.0648     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0327     |\n",
      "|    value_loss           | 0.00116     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.99        |\n",
      "|    ep_rew_mean          | 0.92        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098518185 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.373      |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.0462     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 0.000754    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 7.27      |\n",
      "|    ep_rew_mean          | 0.927     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 97        |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 84        |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1499367 |\n",
      "|    clip_fraction        | 0.101     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.177    |\n",
      "|    explained_variance   | 0.815     |\n",
      "|    learning_rate        | 0.00141   |\n",
      "|    loss                 | -0.0425   |\n",
      "|    n_updates            | 70        |\n",
      "|    policy_gradient_loss | -0.0355   |\n",
      "|    value_loss           | 0.00014   |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.28        |\n",
      "|    ep_rew_mean          | 0.927       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006753622 |\n",
      "|    clip_fraction        | 0.0369      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0988     |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.0354     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 4.91e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.03        |\n",
      "|    ep_rew_mean          | 0.93        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029381424 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0377     |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 4.7e-05     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 7.05         |\n",
      "|    ep_rew_mean          | 0.93         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 11264        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017151742 |\n",
      "|    clip_fraction        | 0.00439      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0242      |\n",
      "|    explained_variance   | 0.983        |\n",
      "|    learning_rate        | 0.00141      |\n",
      "|    loss                 | 0.000193     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00301     |\n",
      "|    value_loss           | 1.1e-05      |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.04       |\n",
      "|    ep_rew_mean          | 0.93       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 98         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 124        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00699903 |\n",
      "|    clip_fraction        | 0.00742    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00889   |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.00141    |\n",
      "|    loss                 | -0.00187   |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0077    |\n",
      "|    value_loss           | 1.08e-05   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 7            |\n",
      "|    ep_rew_mean          | 0.93         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 134          |\n",
      "|    total_timesteps      | 13312        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034964834 |\n",
      "|    clip_fraction        | 0.00283      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00258     |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.00141      |\n",
      "|    loss                 | -0.000464    |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00358     |\n",
      "|    value_loss           | 5.77e-06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 7             |\n",
      "|    ep_rew_mean          | 0.93          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 99            |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 144           |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7169514e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00393      |\n",
      "|    explained_variance   | 1             |\n",
      "|    learning_rate        | 0.00141       |\n",
      "|    loss                 | -0.00136      |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -0.000317     |\n",
      "|    value_loss           | 8.09e-09      |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.35       |\n",
      "|    ep_rew_mean          | 0.907      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 99         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 154        |\n",
      "|    total_timesteps      | 15360      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03116519 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.133     |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.00141    |\n",
      "|    loss                 | -0.0675    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0333    |\n",
      "|    value_loss           | 1.09e-12   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.74        |\n",
      "|    ep_rew_mean          | 0.923       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033392034 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.309      |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.0472     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0379     |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.21        |\n",
      "|    ep_rew_mean          | 0.928       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045084596 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.111      |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.024      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.000243    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.09        |\n",
      "|    ep_rew_mean          | 0.929       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 184         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008790848 |\n",
      "|    clip_fraction        | 0.0234      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0619     |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00913    |\n",
      "|    value_loss           | 6.66e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 7.11         |\n",
      "|    ep_rew_mean          | 0.929        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 194          |\n",
      "|    total_timesteps      | 19456        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028526476 |\n",
      "|    clip_fraction        | 0.0123       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0377      |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.00141      |\n",
      "|    loss                 | -0.00092     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00744     |\n",
      "|    value_loss           | 1.4e-05      |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7          |\n",
      "|    ep_rew_mean          | 0.93       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 100        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 204        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00892067 |\n",
      "|    clip_fraction        | 0.0144     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0175    |\n",
      "|    explained_variance   | 0.937      |\n",
      "|    learning_rate        | 0.00141    |\n",
      "|    loss                 | -0.0131    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.00892   |\n",
      "|    value_loss           | 4.64e-05   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1f93bd77130>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac061ff",
   "metadata": {},
   "source": [
    "The training is now stable and our agent has learned to maximize its reward as can be seen from the high ep_rew_mean which is the average reward the agent got per episode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2a5b0",
   "metadata": {},
   "source": [
    "### Test and evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a06bd1",
   "metadata": {},
   "source": [
    "We will evaluate our model in the video game environment to determine its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9a32698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import evaluate policy to test our agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf11f0d",
   "metadata": {},
   "source": [
    "We load our best trained model to use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "157c5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./train/train_Ubasic3/best_model_20000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91bd03",
   "metadata": {},
   "source": [
    "Create and render our environment to see the agents performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d9a933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 1.5.0-preview and communication version 1.2.0\n",
      "[INFO] Connected new brain: Basic?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jolomi\\anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "env = UnityGymBasic(render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ff17a36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 0.9300000108778477 is 0\n",
      "Total Reward for episode 0.9300000108778477 is 1\n",
      "Total Reward for episode 0.9300000108778477 is 2\n",
      "Total Reward for episode 0.9300000108778477 is 3\n",
      "Total Reward for episode 0.9300000108778477 is 4\n",
      "Total Reward for episode 0.9300000108778477 is 5\n",
      "Total Reward for episode 0.9300000108778477 is 6\n",
      "Total Reward for episode 0.9300000108778477 is 7\n",
      "Total Reward for episode 0.9300000108778477 is 8\n",
      "Total Reward for episode 0.9300000108778477 is 9\n",
      "Total Reward for episode 0.9300000108778477 is 10\n",
      "Total Reward for episode 0.9300000108778477 is 11\n",
      "Total Reward for episode 0.9300000108778477 is 12\n",
      "Total Reward for episode 0.9300000108778477 is 13\n",
      "Total Reward for episode 0.9300000108778477 is 14\n",
      "Total Reward for episode 0.9300000108778477 is 15\n",
      "Total Reward for episode 0.9300000108778477 is 16\n",
      "Total Reward for episode 0.9300000108778477 is 17\n",
      "Total Reward for episode 0.9300000108778477 is 18\n",
      "Total Reward for episode 0.9300000108778477 is 19\n",
      "Total Reward for episode 0.9300000108778477 is 20\n",
      "Total Reward for episode 0.9300000108778477 is 21\n",
      "Total Reward for episode 0.9300000108778477 is 22\n",
      "Total Reward for episode 0.9300000108778477 is 23\n",
      "Total Reward for episode 0.9300000108778477 is 24\n",
      "Total Reward for episode 0.9300000108778477 is 25\n",
      "Total Reward for episode 0.9300000108778477 is 26\n",
      "Total Reward for episode 0.9300000108778477 is 27\n",
      "Total Reward for episode 0.9300000108778477 is 28\n",
      "Total Reward for episode 0.9300000108778477 is 29\n"
     ]
    }
   ],
   "source": [
    "#loop through each game\n",
    "for episode in range(30):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.20)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(total_reward, episode))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e043b0",
   "metadata": {},
   "source": [
    "The agent is fully trained and performs very well as seen from the high rewards above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac711d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f32513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
