{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abce5950",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8278d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union, Any\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import VecTransposeImage\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b75f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd github & git clone https://github.com/mwydmuch/ViZDoom.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75813f9",
   "metadata": {},
   "source": [
    "For this project we will be making use of openai gym to train our agents in a videogame called Doom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf16cd3",
   "metadata": {},
   "source": [
    "### Importing  OpenAI and Doom Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd58c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c931926",
   "metadata": {},
   "source": [
    "### Game Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "144518c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a game instance\n",
    "game = DoomGame()\n",
    "#Load our desired Doom configurations \n",
    "game.load_config('github/ViZDoom/scenarios/basic.cfg')\n",
    "#starting up the game\n",
    "#game.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8470a8",
   "metadata": {},
   "source": [
    "To be able to train our agent in the DOOM videogame we will first need to \"wrap\" the game with openai gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7f7ee",
   "metadata": {},
   "source": [
    "This enables us to collect observations from each game frameshot and pass it to our agent. It also enables our agent to perform specified action in the game world which will be how our agent learns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed739e",
   "metadata": {},
   "source": [
    "### Wrapping and defining our environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc9c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our vizdoom environment class\n",
    "class VizDoomGym(Env):\n",
    "    #Initialize our environment\n",
    "    def __init__(self, render=False):\n",
    "        #inherit from env base class\n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "        #This allows us to load up our configurations which defines our maps,rewards,buttons etc...\n",
    "        self.game.load_config('github/ViZDoom/scenarios/basic.cfg')\n",
    "        \n",
    "        \n",
    "        #Determine if to render game window\n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        #start up the game\n",
    "        self.game.init()\n",
    "        \n",
    "        #create our observation space.\n",
    "        #We want the same of the observation space to match the game frame exactly- \n",
    "        #This is what is used to establish the parameters for the underlying models.\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8)\n",
    "        #define our action space\n",
    "        self.action_space = Discrete(3)\n",
    "    #take step in environment\n",
    "    def step(self, action):\n",
    "        #Define the action to take\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        #this actions will be a matrix defining if the agent go left,\n",
    "        #right or shoot and also our frame skip parameter\n",
    "        reward = self.game.make_action(actions[action],4)        \n",
    "        \n",
    "        #return numpy zeroes array if nothing is returned\n",
    "        if self.game.get_state():\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            #gray scaling the captured image\n",
    "            state = self.grayscale(state)\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            info = ammo\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0      \n",
    "        \n",
    "        \n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info\n",
    "    #render game \n",
    "    def render():\n",
    "        pass    \n",
    "    def reset(self):\n",
    "        self.game.new_episode()        \n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    #grayscale and resize the image to improve training performance\n",
    "    def grayscale(self, observation):\n",
    "        #take the observation, grab the color channel and move it to the end\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "    #close the game\n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6bcd92",
   "metadata": {},
   "source": [
    "### Verify our environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d9714",
   "metadata": {},
   "source": [
    "We check our wrapped environment possible errors using stable baselines3 env_checker to ensure our agent does not run into issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d4e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import environment checker\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c981e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f9fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46e0baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f787ff",
   "metadata": {},
   "source": [
    "### Defining our Custom Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0983865",
   "metadata": {},
   "source": [
    "Creating a neural network for our Actor and Critic network, this will be the brains of our agent enabling it to learn and take action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "886d7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork(nn.Module):\n",
    "  \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 128,\n",
    "        last_layer_dim_vf: int = 128,        \n",
    "    ):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi        \n",
    "        self.latent_dim_vf = last_layer_dim_vf      \n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()            \n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()            \n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \n",
    "        return self.policy_net(features), self.value_net(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37236afc",
   "metadata": {},
   "source": [
    "### Defining our Custom Actor-Critic Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0941e",
   "metadata": {},
   "source": [
    "We create policies for the actor-critic network, here we can specify the number of neurons and size of our the hidden layers\n",
    "in our networks using net_arch and how many are shared amoung the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6765f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: gym.spaces.Space,\n",
    "        action_space: gym.spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n",
    "        #net_arch = [dict(pi=[32, 32, 32], vf=[32, 32, 32])],\n",
    "        #activation_fn: Type[nn.Module] = th.nn.ReLU,\n",
    "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super(CustomActorCriticPolicy, self).__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Disable orthogonal initialization\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60300aaa",
   "metadata": {},
   "source": [
    "### Setting up Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f9a85",
   "metadata": {},
   "source": [
    " We pass in how frequently we want to save our model, were we are going to be saving it and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3d6d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAndLoggingCallback(BaseCallback):\n",
    "    \n",
    "   \n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainingAndLoggingCallback, self). __init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54128bb7",
   "metadata": {},
   "source": [
    "Create folders for saving our models and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6979927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_basic2'\n",
    "LOG_DIR = './train/log_basic'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c461460",
   "metadata": {},
   "source": [
    "Create an instance of our train and logging callbacks. After the specified training steps, we will save the best version of our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32267deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainingAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e47314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed002f8",
   "metadata": {},
   "source": [
    "### Train our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7fde2b",
   "metadata": {},
   "source": [
    "Create the video game environment without rendering it to save compute power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55fb55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089cba9",
   "metadata": {},
   "source": [
    "We now create our model using our custom neural network and policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1ea4111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(CustomActorCriticPolicy, env, tensorboard_log=LOG_DIR, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bde2ab",
   "metadata": {},
   "source": [
    "Train our model using a specified number of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d506323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./train/log_basic\\PPO_31\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.3     |\n",
      "|    ep_rew_mean     | -78.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 172      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 32           |\n",
      "|    ep_rew_mean          | -77.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 143          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070503624 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -0.000728    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 981          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 2.47e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 28.1        |\n",
      "|    ep_rew_mean          | -55         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009847812 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.4e+03     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00557    |\n",
      "|    value_loss           | 3.49e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.5        |\n",
      "|    ep_rew_mean          | -65.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008536143 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.92e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00772    |\n",
      "|    value_loss           | 4.45e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 31.5        |\n",
      "|    ep_rew_mean          | -80         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008580585 |\n",
      "|    clip_fraction        | 0.0643      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0895      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.11e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00746    |\n",
      "|    value_loss           | 5.2e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | -78         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003230917 |\n",
      "|    clip_fraction        | 0.00356     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.75e+03    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    value_loss           | 4.15e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 32.8        |\n",
      "|    ep_rew_mean          | -88.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009307135 |\n",
      "|    clip_fraction        | 0.0415      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.74e+03    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00383    |\n",
      "|    value_loss           | 4.36e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.7        |\n",
      "|    ep_rew_mean          | -55.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007683043 |\n",
      "|    clip_fraction        | 0.0466      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.45e+03    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00407    |\n",
      "|    value_loss           | 4.95e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 21.8         |\n",
      "|    ep_rew_mean          | -21.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 139          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054457793 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.254        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.23e+03     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00348     |\n",
      "|    value_loss           | 4.67e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 23.4         |\n",
      "|    ep_rew_mean          | -28.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 152          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071420316 |\n",
      "|    clip_fraction        | 0.0234       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.364        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.57e+03     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 4.14e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.2        |\n",
      "|    ep_rew_mean          | -51.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010223103 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.09e+03    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 4.01e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.1        |\n",
      "|    ep_rew_mean          | -33.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011825504 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.53e+03    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 3.42e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 22.2       |\n",
      "|    ep_rew_mean          | -20.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 200        |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00775264 |\n",
      "|    clip_fraction        | 0.0722     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0.463      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.15e+03   |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00352   |\n",
      "|    value_loss           | 3.66e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.6        |\n",
      "|    ep_rew_mean          | -35.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011194112 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.18e+03    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00535    |\n",
      "|    value_loss           | 3.84e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.4        |\n",
      "|    ep_rew_mean          | -54.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008353248 |\n",
      "|    clip_fraction        | 0.0777      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.85e+03    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00394    |\n",
      "|    value_loss           | 3.46e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.1        |\n",
      "|    ep_rew_mean          | -44.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 246         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006459607 |\n",
      "|    clip_fraction        | 0.082       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.04e+03    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    value_loss           | 3.5e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 17.5        |\n",
      "|    ep_rew_mean          | 2.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 262         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009341741 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.553       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.6e+03     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00262    |\n",
      "|    value_loss           | 3.32e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 20.8         |\n",
      "|    ep_rew_mean          | -20.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 279          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102310395 |\n",
      "|    clip_fraction        | 0.159        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.99        |\n",
      "|    explained_variance   | 0.587        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.58e+03     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00798     |\n",
      "|    value_loss           | 4.26e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 21.4         |\n",
      "|    ep_rew_mean          | -24.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 294          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056845816 |\n",
      "|    clip_fraction        | 0.0762       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.978       |\n",
      "|    explained_variance   | 0.56         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.04e+03     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 3.98e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 27.8         |\n",
      "|    ep_rew_mean          | -65.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 311          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035392009 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.987       |\n",
      "|    explained_variance   | 0.692        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.92e+03     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    value_loss           | 2.92e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1c35c6bc970>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f037358",
   "metadata": {},
   "source": [
    "The model appears to be very unstable and unable to learn. We can imporve our model by altering between various hyperparameters like the learning rate, gamma and n_steps, we can also manually alter the number of neurons and hidden layers in our networks to find the best combination. \n",
    "\n",
    "However, doing all of this manually is going to be very time consuming and tedious. So, we will automate our hyperparameter search using Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece442b1",
   "metadata": {},
   "source": [
    "## Optuna for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241b37b",
   "metadata": {},
   "source": [
    "### Dependencies for optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ed3de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd2e6c",
   "metadata": {},
   "source": [
    "### Optuna variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190fff5",
   "metadata": {},
   "source": [
    "We specify variables used by optuna, including the number of trails to be performed to find the ideal hyperparameters, policy etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4df22450",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 100\n",
    "N_STARTUP_TRIALS = 10\n",
    "N_EVALUATIONS = 2\n",
    "N_TIMESTEPS = int(2e4)\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_EPISODES = 3\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"CnnPolicy\",\n",
    "    \"env\": env,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ff87b",
   "metadata": {},
   "source": [
    "### Optuna Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af6726c",
   "metadata": {},
   "source": [
    "In our sampler we specify the range of values for the various hyperparameters that Optuna is going to search in order to find an ideal combination that yields a stable agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ee1cf",
   "metadata": {},
   "source": [
    "We also specify various configurations of neural networks for Optuna to search, this includes different number of neurons and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1127a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_PPO_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "    gae_lambda = 1.0 - trial.suggest_float(\"gae_lambda\", 0.001, 0.2, log=True)\n",
    "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n",
    "    ortho_init = trial.suggest_categorical(\"ortho_init\", [False, True])\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"gae_lambda_\", gae_lambda)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    net_arch = [\n",
    "        {\"pi\": [32], \"vf\": [32]} if net_arch == \"tiny\"\n",
    "        else {\"pi\": [32, 32], \"vf\": [32, 32]}   \n",
    "    ]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "            \"ortho_init\": ortho_init,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "# policy_kwargs1 = dict(\n",
    "#     features_extractor_class=CustomCNN,\n",
    "#     features_extractor_kwargs=dict(features_dim=128),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc937f6",
   "metadata": {},
   "source": [
    "### Callback for evaluating trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeca2ad",
   "metadata": {},
   "source": [
    "Callback to be used in evaluating and testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5a20b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialEvalCallback(EvalCallback):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833b48b",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956becdc",
   "metadata": {},
   "source": [
    "Using how specified hyperparameter and neural network architecture ranges, Optuna will not run a number of trials to find the best combination that produces the best possible agent.\n",
    "\n",
    "After a stabel agent is found and Optuna determines that a better model is no longer possible, the trail is stopped and the  hyperparameters and network architecture values of the best model is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "554e7400",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-05 23:03:17,820]\u001b[0m A new study created in memory with name: no-name-03bbfd08-7b5d-4554-8a64-07584f97d4c2\u001b[0m\n",
      "C:\\Users\\Jolomi\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:137: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=8 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-04-05 23:13:16,714]\u001b[0m Trial 0 finished with value: -300.0 and parameters: {'gamma': 0.0007051797960307834, 'max_grad_norm': 4.5512822194466525, 'gae_lambda': 0.015912329953409167, 'exponent_n_steps': 3, 'lr': 0.002442977850335494, 'ent_coef': 3.644398966552039e-08, 'ortho_init': True, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 0 with value: -300.0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-05 23:20:18,035]\u001b[0m Trial 1 finished with value: 93.66666666666667 and parameters: {'gamma': 0.08281418726849317, 'max_grad_norm': 1.5061594927147473, 'gae_lambda': 0.0026364591694504735, 'exponent_n_steps': 10, 'lr': 0.000459048881080786, 'ent_coef': 2.4386279564585447e-08, 'ortho_init': True, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 1 with value: 93.66666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  2\n",
      "Best trial:\n",
      "  Value:  93.66666666666667\n",
      "  Params: \n",
      "    gamma: 0.08281418726849317\n",
      "    max_grad_norm: 1.5061594927147473\n",
      "    gae_lambda: 0.0026364591694504735\n",
      "    exponent_n_steps: 10\n",
      "    lr: 0.000459048881080786\n",
      "    ent_coef: 2.4386279564585447e-08\n",
      "    ortho_init: True\n",
      "    net_arch: small\n",
      "    activation_fn: relu\n",
      "  User attrs:\n",
      "    gamma_: 0.9171858127315068\n",
      "    gae_lambda_: 0.9973635408305496\n",
      "    n_steps: 1024\n"
     ]
    }
   ],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    # Sample hyperparameters\n",
    "    kwargs.update(sample_PPO_params(trial))\n",
    "    # Create our model\n",
    "    model = PPO(**kwargs)\n",
    "    # Wrapping env used for evaluation\n",
    "    eval_env = Monitor(env)\n",
    "    eval_env = DummyVecEnv([lambda: eval_env])\n",
    "    eval_env = VecTransposeImage(eval_env)    \n",
    "    # Create the callback that will periodically evaluate\n",
    "    # and report the performance\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set pytorch num threads to 1 for faster training\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "    # Do not prune before 1/3 of the max budget is used\n",
    "    pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3)\n",
    "\n",
    "    study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=N_TRIALS, timeout=600)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    print(\"  User attrs:\")\n",
    "    for key, value in trial.user_attrs.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed1d36c",
   "metadata": {},
   "source": [
    "A stable model was found after only two trails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46ab0e",
   "metadata": {},
   "source": [
    "### Create and train our tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225db16b",
   "metadata": {},
   "source": [
    "Having completed our hyperparameter search, we plug in the hyperparameters generated from the search into our model along with the network archtecture values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807588e",
   "metadata": {},
   "source": [
    "We supply the searched network architecture, orth_init and activation function using policy kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7248eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs2 = dict(\n",
    "    ortho_init = True,\n",
    "    activation_fn = th.nn.ReLU,\n",
    "    net_arch = [dict(pi=[32, 32], vf=[32, 32])],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c971b06",
   "metadata": {},
   "source": [
    "Create our model using searched hyperparameters and policy kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e0e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"CnnPolicy\", env, tensorboard_log=LOG_DIR, verbose=1,\n",
    "           gamma = 0.9171858127315068, max_grad_norm = 1.5061594927147473,\n",
    "           gae_lambda = 0.9973635408305496, learning_rate = 0.000459048881080786,\n",
    "           ent_coef = 2.4386279564585447e-08, policy_kwargs=policy_kwargs2, n_steps = 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7151eb",
   "metadata": {},
   "source": [
    "Train our agent using a specified number of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39cd6fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./train/log_basic\\PPO_40\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.9     |\n",
      "|    ep_rew_mean     | 38.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 98       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.93       |\n",
      "|    ep_rew_mean          | 73         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 79         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 2048       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07594001 |\n",
      "|    clip_fraction        | 0.382      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.662     |\n",
      "|    explained_variance   | 0.584      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 90.4       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | 0.00254    |\n",
      "|    value_loss           | 388        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.52       |\n",
      "|    ep_rew_mean          | 80.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 66         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 46         |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04965913 |\n",
      "|    clip_fraction        | 0.435      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.616     |\n",
      "|    explained_variance   | 0.721      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 185        |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.0292    |\n",
      "|    value_loss           | 590        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.53       |\n",
      "|    ep_rew_mean          | 84.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 66         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07275669 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.425     |\n",
      "|    explained_variance   | 0.468      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 61.2       |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    value_loss           | 261        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.69       |\n",
      "|    ep_rew_mean          | 83.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 62         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 82         |\n",
      "|    total_timesteps      | 5120       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09613909 |\n",
      "|    clip_fraction        | 0.22       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.292     |\n",
      "|    explained_variance   | 0.703      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 36.5       |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.00806   |\n",
      "|    value_loss           | 98.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.39        |\n",
      "|    ep_rew_mean          | 89.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026881834 |\n",
      "|    clip_fraction        | 0.0926      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.624       |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | 48.6        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 180         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.86        |\n",
      "|    ep_rew_mean          | 87.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052020796 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.102      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | 18.5        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.0166      |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.72        |\n",
      "|    ep_rew_mean          | 84          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093100175 |\n",
      "|    clip_fraction        | 0.0724      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0798     |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | 4.09        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00399    |\n",
      "|    value_loss           | 13.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.83        |\n",
      "|    ep_rew_mean          | 83.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061666433 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | 0.000291    |\n",
      "|    value_loss           | 80          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.82       |\n",
      "|    ep_rew_mean          | 87.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 166        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16479266 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0601    |\n",
      "|    explained_variance   | 0.901      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 12.9       |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    value_loss           | 32.8       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.01       |\n",
      "|    ep_rew_mean          | 86.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 183        |\n",
      "|    total_timesteps      | 11264      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09675221 |\n",
      "|    clip_fraction        | 0.0638     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.046     |\n",
      "|    explained_variance   | 0.788      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 32.9       |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | 0.0228     |\n",
      "|    value_loss           | 74.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.36       |\n",
      "|    ep_rew_mean          | 89.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 198        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16132867 |\n",
      "|    clip_fraction        | 0.064      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0372    |\n",
      "|    explained_variance   | 0.874      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 8.82       |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0033    |\n",
      "|    value_loss           | 31.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.26       |\n",
      "|    ep_rew_mean          | 85.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 62         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 214        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03704025 |\n",
      "|    clip_fraction        | 0.0444     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0405    |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 1.45       |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | 0.0156     |\n",
      "|    value_loss           | 14.7       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.77       |\n",
      "|    ep_rew_mean          | 87.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 62         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 230        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06985583 |\n",
      "|    clip_fraction        | 0.0715     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0509    |\n",
      "|    explained_variance   | 0.769      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 13.6       |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.00553   |\n",
      "|    value_loss           | 79.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.88       |\n",
      "|    ep_rew_mean          | 87.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 62         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 246        |\n",
      "|    total_timesteps      | 15360      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03757565 |\n",
      "|    clip_fraction        | 0.0332     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.034     |\n",
      "|    explained_variance   | 0.876      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 20.7       |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | 0.00463    |\n",
      "|    value_loss           | 36.2       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.36       |\n",
      "|    ep_rew_mean          | 85.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 62         |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 263        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07537817 |\n",
      "|    clip_fraction        | 0.0485     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0276    |\n",
      "|    explained_variance   | 0.774      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 75.6       |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    value_loss           | 88.7       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.87       |\n",
      "|    ep_rew_mean          | 87.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 281        |\n",
      "|    total_timesteps      | 17408      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06296997 |\n",
      "|    clip_fraction        | 0.0538     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0518    |\n",
      "|    explained_variance   | 0.885      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 4.82       |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | 0.00315    |\n",
      "|    value_loss           | 25.4       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.1        |\n",
      "|    ep_rew_mean          | 86.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 60         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 302        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12879433 |\n",
      "|    clip_fraction        | 0.0709     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0473    |\n",
      "|    explained_variance   | 0.945      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 3.26       |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | 0.00321    |\n",
      "|    value_loss           | 16.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.22        |\n",
      "|    ep_rew_mean          | 85.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040342927 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0423     |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.000459    |\n",
      "|    loss                 | 24.5        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | 0.00734     |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.16       |\n",
      "|    ep_rew_mean          | 86.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 58         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 351        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15453027 |\n",
      "|    clip_fraction        | 0.056      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0325    |\n",
      "|    explained_variance   | 0.803      |\n",
      "|    learning_rate        | 0.000459   |\n",
      "|    loss                 | 11.5       |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.00124   |\n",
      "|    value_loss           | 45.3       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2ca14cfcf40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac061ff",
   "metadata": {},
   "source": [
    "The training is now stable and our agent has learned to maximize its reward as can be seen from the high ep_rew_mean which is the average reward the agent got per episode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2a5b0",
   "metadata": {},
   "source": [
    "### Test and evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a06bd1",
   "metadata": {},
   "source": [
    "We will evaluate our model in the video game environment to determine its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a32698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import evaluate policy to test our agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf11f0d",
   "metadata": {},
   "source": [
    "We load our best trained model to use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "157c5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./train/train_basic2/best_model_80000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91bd03",
   "metadata": {},
   "source": [
    "Create and render our environment to see the agents performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4d9a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ff17a36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 71.0 is 0\n",
      "Total Reward for episode 95.0 is 1\n",
      "Total Reward for episode 95.0 is 2\n",
      "Total Reward for episode 95.0 is 3\n",
      "Total Reward for episode 95.0 is 4\n",
      "Total Reward for episode 95.0 is 5\n",
      "Total Reward for episode 95.0 is 6\n",
      "Total Reward for episode 83.0 is 7\n",
      "Total Reward for episode 95.0 is 8\n",
      "Total Reward for episode 79.0 is 9\n"
     ]
    }
   ],
   "source": [
    "#loop through each game\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.20)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(total_reward, episode))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e043b0",
   "metadata": {},
   "source": [
    "The agent is fully trained and performs very well as seen from the high rewards above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac711d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f32513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
