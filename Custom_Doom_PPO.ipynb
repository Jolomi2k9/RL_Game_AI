{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abce5950",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8278d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b75f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd github & git clone https://github.com/mwydmuch/ViZDoom.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf16cd3",
   "metadata": {},
   "source": [
    "### Importing  OpenAI and Doom Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd58c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c931926",
   "metadata": {},
   "source": [
    "### Game Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144518c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a game instance\n",
    "game = DoomGame()\n",
    "#Load our desired Doom configurations \n",
    "game.load_config('github/ViZDoom/scenarios/basic.cfg')\n",
    "#starting up the game\n",
    "#game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6f8629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed739e",
   "metadata": {},
   "source": [
    "### Wrapping and defining our environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebc9c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our vizdoom environment class\n",
    "class VizDoomGym(Env):\n",
    "    #Initialize our environment\n",
    "    def __init__(self, render=False):\n",
    "        #inherit from env base class\n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "        #This allows us to load up our configurations which defines our maps,rewards,buttons etc...\n",
    "        self.game.load_config('github/ViZDoom/scenarios/basic.cfg')\n",
    "        \n",
    "        \n",
    "        #Determine if to render game window\n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        #start up the game\n",
    "        self.game.init()\n",
    "        \n",
    "        #create our observation space.\n",
    "        #We want the same of the observation space to match the game frame exactly- \n",
    "        #This is what is used to establish the parameters for the underlying models.\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8)\n",
    "        #define our action space\n",
    "        self.action_space = Discrete(3)\n",
    "    #take step in environment\n",
    "    def step(self, action):\n",
    "        #Define the action to take\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        #this actions will be a matrix defining if the agent go left,\n",
    "        #right or shoot and also our frame skip parameter\n",
    "        reward = self.game.make_action(actions[action],4)        \n",
    "        \n",
    "        #return numpy zeroes array if nothing is returned\n",
    "        if self.game.get_state():\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            #gray scaling the captured image\n",
    "            state = self.grayscale(state)\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            info = ammo\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0      \n",
    "        \n",
    "        \n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info\n",
    "    #render game \n",
    "    def render():\n",
    "        pass    \n",
    "    def reset(self):\n",
    "        self.game.new_episode()        \n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    #grayscale and resize the image\n",
    "    def grayscale(self, observation):\n",
    "        #take the observation, grab the color channel and move it to the end\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "    #close the game\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6bcd92",
   "metadata": {},
   "source": [
    "### Verify our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d4e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import environment checker\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c981e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65f9fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46e0baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f787ff",
   "metadata": {},
   "source": [
    "### Defining our Custom Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "886d7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork(nn.Module):\n",
    "  \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 64,\n",
    "        last_layer_dim_vf: int = 64,\n",
    "    ):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()\n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \n",
    "        return self.policy_net(features), self.value_net(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37236afc",
   "metadata": {},
   "source": [
    "### Defining our Custom Actor-Critic Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6765f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: gym.spaces.Space,\n",
    "        action_space: gym.spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super(CustomActorCriticPolicy, self).__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Disable orthogonal initialization\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60300aaa",
   "metadata": {},
   "source": [
    "### Setting up Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3d6d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAndLoggingCallback(BaseCallback):\n",
    "    \n",
    "    #We pass in how frequently we want to save our model, were we are-\n",
    "    #going to be saving it and logs\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainingAndLoggingCallback, self). __init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6979927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create nessesary folders\n",
    "CHECKPOINT_DIR = './train/train_basic'\n",
    "LOG_DIR = './train/log_basic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32267deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of our train and logging callbacks\n",
    "#after the stated steps of training our model, we will save a version of it\n",
    "callback = TrainingAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed002f8",
   "metadata": {},
   "source": [
    "### Train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55fb55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our environment without rendering it\n",
    "env = VizDoomGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5d2a8b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "#Create our model using our custom actor critic policy and NN\n",
    "model = PPO(CustomActorCriticPolicy, env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d506323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./train/log_basic\\PPO_3\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.7     |\n",
      "|    ep_rew_mean     | -125     |\n",
      "| time/              |          |\n",
      "|    fps             | 234      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 38          |\n",
      "|    ep_rew_mean          | -125        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007983889 |\n",
      "|    clip_fraction        | 0.0736      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.000134   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 938         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    value_loss           | 1.89e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.4        |\n",
      "|    ep_rew_mean          | -70.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 148         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012064649 |\n",
      "|    clip_fraction        | 0.0318      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.0448      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.58e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.000686    |\n",
      "|    value_loss           | 3.1e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 28.4        |\n",
      "|    ep_rew_mean          | -60.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007413846 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0985      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.59e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.000924   |\n",
      "|    value_loss           | 5.23e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | -72         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008029526 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.0991      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.66e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00435    |\n",
      "|    value_loss           | 5.36e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30.4         |\n",
      "|    ep_rew_mean          | -77.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 143          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069294283 |\n",
      "|    clip_fraction        | 0.0157       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.166        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.09e+03     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    value_loss           | 4.63e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.5        |\n",
      "|    ep_rew_mean          | -74.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008445792 |\n",
      "|    clip_fraction        | 0.013       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.231       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.98e+03    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    value_loss           | 4.88e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.7        |\n",
      "|    ep_rew_mean          | -11.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 117         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012796227 |\n",
      "|    clip_fraction        | 0.0692      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.11e+03    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00523    |\n",
      "|    value_loss           | 5.69e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.6        |\n",
      "|    ep_rew_mean          | -45         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013681545 |\n",
      "|    clip_fraction        | 0.0579      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.995      |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.25e+03    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00539    |\n",
      "|    value_loss           | 5.55e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20.1        |\n",
      "|    ep_rew_mean          | -16.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011422625 |\n",
      "|    clip_fraction        | 0.0566      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.907      |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.72e+03    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00416    |\n",
      "|    value_loss           | 4.24e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 25.4         |\n",
      "|    ep_rew_mean          | -49.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 139          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 161          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036789463 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.913       |\n",
      "|    explained_variance   | 0.353        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.4e+03      |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    value_loss           | 5.34e+03     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 22.2       |\n",
      "|    ep_rew_mean          | -31.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 141        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 173        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01292056 |\n",
      "|    clip_fraction        | 0.0936     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.86      |\n",
      "|    explained_variance   | 0.374      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.25e+03   |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.00762   |\n",
      "|    value_loss           | 4.35e+03   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 20.8         |\n",
      "|    ep_rew_mean          | -23.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 140          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 189          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036200823 |\n",
      "|    clip_fraction        | 0.00771      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.867       |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.55e+03     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | 4.17e-05     |\n",
      "|    value_loss           | 4.38e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.5        |\n",
      "|    ep_rew_mean          | -55.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 207         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006389289 |\n",
      "|    clip_fraction        | 0.0406      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.896      |\n",
      "|    explained_variance   | 0.539       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.79e+03    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00301    |\n",
      "|    value_loss           | 3.66e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 22.1        |\n",
      "|    ep_rew_mean          | -33.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004909366 |\n",
      "|    clip_fraction        | 0.0355      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.854      |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.75e+03    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00283    |\n",
      "|    value_loss           | 3.6e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23          |\n",
      "|    ep_rew_mean          | -39.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 233         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010328077 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.789      |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.65e+03    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    value_loss           | 3.74e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26.5        |\n",
      "|    ep_rew_mean          | -62.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 247         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006638701 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.745      |\n",
      "|    explained_variance   | 0.599       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.38e+03    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    value_loss           | 2.84e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 28.8         |\n",
      "|    ep_rew_mean          | -76.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 141          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 261          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016979789 |\n",
      "|    clip_fraction        | 0.036        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.766       |\n",
      "|    explained_variance   | 0.582        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.8e+03      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000965    |\n",
      "|    value_loss           | 3.17e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 18.4         |\n",
      "|    ep_rew_mean          | -5.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 142          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 272          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058758575 |\n",
      "|    clip_fraction        | 0.0203       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.772       |\n",
      "|    explained_variance   | 0.591        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12e+03     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 3.29e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 23.4         |\n",
      "|    ep_rew_mean          | -41.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 142          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 286          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031590168 |\n",
      "|    clip_fraction        | 0.0206       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.779       |\n",
      "|    explained_variance   | 0.651        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2e+03        |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000232    |\n",
      "|    value_loss           | 3.73e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1d69756d3a0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train our model\n",
    "model.learn(total_timesteps=40000,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2a5b0",
   "metadata": {},
   "source": [
    "### Test and evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b1b3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import evaluate policy to test our agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "157c5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "model = PPO.load('./train/train_basic/best_model_100000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4d9a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our environment with rendering to test model\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ff17a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 95.0 is 0\n",
      "Total Reward for episode 9.0 is 1\n",
      "Total Reward for episode 95.0 is 2\n",
      "Total Reward for episode -395.0 is 3\n",
      "Total Reward for episode 95.0 is 4\n"
     ]
    }
   ],
   "source": [
    "#loop through each game\n",
    "for episode in range(5):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.20)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(total_reward, episode))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac711d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7b77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
